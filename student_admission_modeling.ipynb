{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8691fb",
   "metadata": {},
   "source": [
    "# Student Admission Prediction - BentoML Project\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline for predicting university admission chances using student data. We will:\n",
    "\n",
    "1. **Load and explore** the processed dataset\n",
    "2. **Build a linear regression model** to predict admission chances\n",
    "3. **Evaluate model performance** using various metrics\n",
    "4. **Prepare the model** for BentoML deployment\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset contains information about students applying to universities with features like:\n",
    "- GRE Score, TOEFL Score, University Rating\n",
    "- Statement of Purpose (SOP), Letter of Recommendation (LOR)\n",
    "- CGPA, Research Experience\n",
    "- **Target**: Chance of Admit (0-1 probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38001aa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data manipulation, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3339e6",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data\n",
    "\n",
    "Load the preprocessed training and test datasets that were created by our `prepare_data.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data_path = 'data/processed/'\n",
    "\n",
    "print(\"Loading processed datasets...\")\n",
    "X_train = pd.read_csv(os.path.join(data_path, 'X_train.csv'))\n",
    "X_test = pd.read_csv(os.path.join(data_path, 'X_test.csv'))\n",
    "y_train = pd.read_csv(os.path.join(data_path, 'y_train.csv'))\n",
    "y_test = pd.read_csv(os.path.join(data_path, 'y_test.csv'))\n",
    "\n",
    "# Convert y dataframes to series for easier handling\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print(f\"âœ… Data loaded successfully!\")\n",
    "print(f\"Training set: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Test set: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "print(f\"\\nFeatures: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15ecd4",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Visualization\n",
    "\n",
    "Explore the dataset to understand the distribution of features and their relationships with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104abd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"ðŸ“Š TRAINING DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset shape: {X_train.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "print(f\"\\nðŸ“ˆ TARGET VARIABLE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Chance of Admit statistics:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "fig.suptitle('Feature Distributions and Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot distributions of each feature\n",
    "features = X_train.columns\n",
    "for i, feature in enumerate(features):\n",
    "    row, col = i // 4, i % 4\n",
    "    \n",
    "    # Distribution plot\n",
    "    axes[row, col].hist(X_train[feature], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[row, col].set_title(f'{feature} Distribution', fontweight='bold')\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(features) < 8:\n",
    "    fig.delaxes(axes[1, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(f\"\\nðŸ”— FEATURE CORRELATIONS\")\n",
    "print(\"=\" * 50)\n",
    "correlation_with_target = X_train.corrwith(y_train).sort_values(ascending=False)\n",
    "print(\"Correlation with Chance of Admit:\")\n",
    "for feature, corr in correlation_with_target.items():\n",
    "    print(f\"{feature:20}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45548c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = X_train.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots of most correlated features with target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Feature vs Chance of Admit Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "top_features = correlation_with_target.abs().nlargest(6).index\n",
    "for i, feature in enumerate(top_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    \n",
    "    axes[row, col].scatter(X_train[feature], y_train, alpha=0.6, color='coral')\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Chance of Admit')\n",
    "    axes[row, col].set_title(f'{feature} vs Chance of Admit\\n(Correlation: {correlation_with_target[feature]:.4f})')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(X_train[feature], y_train, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[row, col].plot(X_train[feature], p(X_train[feature]), \"b--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e835066",
   "metadata": {},
   "source": [
    "## 4. Linear Regression Model Training\n",
    "\n",
    "Build and train a linear regression model to predict the chance of admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1dbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler for feature normalization\n",
    "print(\"ðŸ”§ FEATURE SCALING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Features scaled successfully!\")\n",
    "print(f\"Original feature range example (GRE_Score): {X_train['GRE_Score'].min():.2f} to {X_train['GRE_Score'].max():.2f}\")\n",
    "print(f\"Scaled feature range (GRE_Score): {X_train_scaled[:, 0].min():.2f} to {X_train_scaled[:, 0].max():.2f}\")\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "print(f\"\\nðŸ¤– MODEL TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"âœ… Linear Regression model trained successfully!\")\n",
    "print(f\"Model coefficients shape: {model.coef_.shape}\")\n",
    "print(f\"Model intercept: {model.intercept_:.6f}\")\n",
    "\n",
    "# Display feature coefficients\n",
    "print(f\"\\nðŸ“Š FEATURE COEFFICIENTS\")\n",
    "print(\"=\" * 50)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': model.coef_,\n",
    "    'Abs_Coefficient': np.abs(model.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638494dd",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Performance Metrics\n",
    "\n",
    "Evaluate the trained model using various regression metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both training and test sets\n",
    "print(\"ðŸŽ¯ MODEL PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"âœ… Predictions completed!\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "print(f\"\\nðŸ“ˆ PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Test metrics\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Create metrics comparison table\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['RÂ² Score', 'MSE', 'RMSE', 'MAE'],\n",
    "    'Training': [train_r2, train_mse, train_rmse, train_mae],\n",
    "    'Test': [test_r2, test_mse, test_rmse, test_mae]\n",
    "})\n",
    "\n",
    "print(\"REGRESSION METRICS COMPARISON:\")\n",
    "print(metrics_df.to_string(index=False, float_format='%.6f'))\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nðŸŽ¯ MODEL INTERPRETATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"â€¢ RÂ² Score on test set: {test_r2:.4f} ({test_r2*100:.2f}% variance explained)\")\n",
    "print(f\"â€¢ RMSE on test set: {test_rmse:.6f} (average prediction error)\")\n",
    "print(f\"â€¢ The model explains {test_r2*100:.2f}% of the variance in admission chances\")\n",
    "\n",
    "if test_r2 > 0.8:\n",
    "    print(\"â€¢ ðŸŸ¢ Excellent model performance!\")\n",
    "elif test_r2 > 0.6:\n",
    "    print(\"â€¢ ðŸŸ¡ Good model performance\")\n",
    "else:\n",
    "    print(\"â€¢ ðŸ”´ Model needs improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Evaluation Visualizations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, color='coral')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'b--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Chance of Admit')\n",
    "axes[0, 0].set_ylabel('Predicted Chance of Admit')\n",
    "axes[0, 0].set_title(f'Actual vs Predicted\\n(RÂ² = {test_r2:.4f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6, color='lightgreen')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Histogram of residuals\n",
    "axes[1, 0].hist(residuals, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Residuals')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature importance plot\n",
    "feature_importance_sorted = feature_importance.sort_values('Abs_Coefficient', ascending=True)\n",
    "axes[1, 1].barh(feature_importance_sorted['Feature'], feature_importance_sorted['Coefficient'], \n",
    "                color='lightcoral', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Coefficient Value')\n",
    "axes[1, 1].set_title('Feature Importance (Coefficients)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: Prediction accuracy distribution\n",
    "print(f\"\\nðŸ“Š PREDICTION ACCURACY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate absolute percentage errors\n",
    "ape = np.abs((y_test - y_test_pred) / y_test) * 100\n",
    "mape = np.mean(ape)\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Predictions within 5% error: {np.sum(ape <= 5)}/{len(ape)} ({np.sum(ape <= 5)/len(ape)*100:.1f}%)\")\n",
    "print(f\"Predictions within 10% error: {np.sum(ape <= 10)}/{len(ape)} ({np.sum(ape <= 10)/len(ape)*100:.1f}%)\")\n",
    "print(f\"Predictions within 15% error: {np.sum(ape <= 15)}/{len(ape)} ({np.sum(ape <= 15)/len(ape)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e3867",
   "metadata": {},
   "source": [
    "## 6. Model Saving and BentoML Preparation\n",
    "\n",
    "Save the trained model and scaler for use with BentoML deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e89d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = 'models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ’¾ SAVING TRAINED MODEL AND SCALER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save the trained linear regression model\n",
    "model_path = os.path.join(models_dir, 'admission_model.joblib')\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = os.path.join(models_dir, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names_path = os.path.join(models_dir, 'feature_names.joblib')\n",
    "joblib.dump(list(X_train.columns), feature_names_path)\n",
    "\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "print(f\"âœ… Scaler saved to: {scaler_path}\")\n",
    "print(f\"âœ… Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# Test loading the model to ensure it works\n",
    "print(f\"\\nðŸ§ª TESTING MODEL LOADING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    loaded_model = joblib.load(model_path)\n",
    "    loaded_scaler = joblib.load(scaler_path)\n",
    "    loaded_features = joblib.load(feature_names_path)\n",
    "    \n",
    "    # Test prediction with loaded model\n",
    "    test_sample = X_test_scaled[:1]\n",
    "    original_pred = model.predict(test_sample)[0]\n",
    "    loaded_pred = loaded_model.predict(test_sample)[0]\n",
    "    \n",
    "    print(f\"Original model prediction: {original_pred:.6f}\")\n",
    "    print(f\"Loaded model prediction: {loaded_pred:.6f}\")\n",
    "    print(f\"Predictions match: {np.allclose(original_pred, loaded_pred)}\")\n",
    "    print(\"âœ… Model loading test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading model: {e}\")\n",
    "\n",
    "# Model summary for BentoML integration\n",
    "print(f\"\\nðŸ“‹ MODEL SUMMARY FOR BENTOML\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model Type: Linear Regression\")\n",
    "print(f\"Features: {len(X_train.columns)} features\")\n",
    "print(f\"Feature Names: {list(X_train.columns)}\")\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Test RÂ² Score: {test_r2:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"Model File: {model_path}\")\n",
    "print(f\"Scaler File: {scaler_path}\")\n",
    "print(f\"Ready for BentoML deployment! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2c3ae",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Model Performance Summary\n",
    "- **Algorithm**: Linear Regression with StandardScaler normalization\n",
    "- **Features**: 7 input features (GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA, Research)\n",
    "- **Target**: Chance of Admit (probability between 0-1)\n",
    "- **Dataset**: 500 samples (400 training, 100 testing)\n",
    "\n",
    "### Key Findings\n",
    "1. **Most Important Features**: The features with highest correlation to admission chances\n",
    "2. **Model Performance**: RÂ² score and RMSE values on test set\n",
    "3. **Prediction Accuracy**: Percentage of predictions within acceptable error ranges\n",
    "\n",
    "### Next Steps for BentoML Deployment\n",
    "1. **Create BentoML Service**: Build the API service using the saved model\n",
    "2. **Define API Endpoints**: Create prediction endpoints for single and batch predictions\n",
    "3. **Build Bento**: Package the service into a deployable bento\n",
    "4. **Containerize**: Create Docker container for deployment\n",
    "5. **Test API**: Validate the deployed service with test data\n",
    "\n",
    "The model and preprocessing components are now ready for BentoML integration! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
